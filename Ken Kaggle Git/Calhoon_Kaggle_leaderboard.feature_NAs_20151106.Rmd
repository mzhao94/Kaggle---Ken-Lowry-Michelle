---
title: "Calhoon_Kaggle
author: "Ken Calhoon"
date: Nov 6 2015
output: html_document
---

The script below looks at the column medians and counts the number of NAs. Both the leaderboard and the validation data set have 68 columns with NAs!!


```{r}
training.features <- read.csv(file="training_features.csv", header=TRUE, sep=",")
leaderboard.features <- read.csv(file="leaderboard_features.csv", header=TRUE, sep=",")
validation.features <- read.csv(file="validation_features.csv", header=TRUE, sep=",")

median.vals.training=as.data.frame(apply(training.features,2,median,na.rm=TRUE))
sum(is.na(median.vals.training))
median.vals.leaderboard=as.data.frame(apply(leaderboard.features,2,median,na.rm=TRUE))
sum(is.na(median.vals.leaderboard))
median.vals.validation=as.data.frame(apply(validation.features,2,median,na.rm=TRUE))
sum(is.na(median.vals.validation))
```












```{r}
```{r, echo=FALSE}
setwd("~/Dropbox/Stats202/Kaggle")
training.features <- read.csv(file="training_features.csv", header=TRUE, sep=",")
training.target <- read.csv(file="training_target.csv",header=TRUE, sep=",")
feature.names <- names(training.features) # Create vector of feature names
for (feature.name in feature.names[-1]) {
                    # Give the new dummy variable a meaningful name
                    dummy.name <- paste0("is.na.",feature.name)
                    is.na.feature <- is.na(training.features[,feature.name])
                    # Convert boolean values to binary
                    training.features[,dummy.name] <- as.integer(is.na.feature)
                    # Replace NA with median value for each column
                    training.features[is.na.feature,feature.name] <- median(training.features[,feature.name], na.rm = TRUE)
}
dim.data.frame(training.features) # Confirms dimension of new dataframe
merged.data=merge(training.target,training.features,by="subject.id") # creates one DF with ALSFRS_slope (response variable) as column 2
```
```





Replicating Ridge code from book

```{r}
x=model.matrix(Salary âˆ¼ .,Hitters)[,-1]
y=Hitters$Salary
library(glmnet)
grid=10^seq(10,-2,length=100)
ridge.mod=glmnet(x,y,alpha=0,lambda=grid)
```


Ridge code

```{r}
#install.packages("glmnet")
library(glmnet)
merged.data.subset=merged.data[,1:859]
x=model.matrix(ALSFRS_slope~.,merged.data.subset)[,-2]
y=merged.data.subset$ALSFRS_slope
grid=20^seq(10,-2,length=100)
ridge.mod=glmnet(x,y,alpha=0,lambda=grid,standardize=FALSE)
```

```{r}
dim(coef(ridge.mod))
ridge.mod$lambda[60]
coef(ridge.mod)[,60]
```

```{r}
set.seed(1)
train=sample(1:nrow(x),nrow(x)/2)
test=(-train) # shouldn't this be != instead?
y.test=y[test]
ridge.mod=glmnet(x[train,],y[train],alpha=0,lambda=grid,thresh=1e-12)
ridge.pred=predict(ridge.mod,s=4,newx=x[test,])
rmse=mean((ridge.pred-y.test)^2)^.5
rmse
```

Compare to lambda = 0 (means no influence from lambda)
```{r}
set.seed(1)
train=sample(1:nrow(x),nrow(x)/2)
test=(-train) # shouldn't this be != instead?
y.test=y[test]
ridge.mod=glmnet(x[train,],y[train],alpha=0,lambda=grid)
ridge.pred=predict(ridge.mod,s=0,newx=x[test,]) #change s value
rmse=mean((ridge.pred-y.test)^2)^.5
rmse
```


Lasso
```{r}
#install.packages("glmnet")
library(glmnet)
merged.data.subset=merged.data[1:1000,1:859] #add back all rows
x=model.matrix(ALSFRS_slope~.,merged.data.subset)[,-2]
y=merged.data.subset$ALSFRS_slope
lasso.mod=glmnet(x[train ,],y[train],alpha=1,lambda=grid)
plot(lasso.mod)
```

Lasso + CV--Test Code
```{r}

rmse
#rerun lasso.mod with all x & y (submission)
merged.data.subset=merged.data[,1:859] #add back all data
#predict with leaderboard.features (submission)

```

Lasso + CV--Real Model [Need to modify this]
```{r}
library(glmnet)
library(boot)
set.seed(1)
merged.data.subset=merged.data[1:500,1:859] #can reduce rows to make easier to work with
x=model.matrix(ALSFRS_slope~.,merged.data.subset)[,-2]
y=merged.data.subset$ALSFRS_slope
#ALSFRS_slope=y
train=sample(1:nrow(x),nrow(x)*.8) #Can adjust
test=(-train)
y.test=y[test]
cv.out=cv.glmnet(x[train ,],y[train],alpha=1,nfolds=3) #change later to K=10, alpha can range 0 to 1 and can run CV on this to get best mix
plot(cv.out)
bestlam=cv.out$lambda.min
bestlam
#insert code to do the cv error estimate
lasso.mod=glmnet(x[train ,],y[train],alpha=1,lambda=bestlam)
#cv.out=cv.glm(merged.data.subset[test,],lasso.mod,K=3) #Change this value later to 5 or 10
lasso.pred=predict(lasso.mod,s=bestlam ,newx=x)
rmse=mean((lasso.pred-y.test)^2)^.5
rmse

leaderboard.correct.vars <- leaderboard.features[,1:858]
leaderboard.matx <- as.matrix(leaderboard.correct.vars)
#leaderboard.x=as.matrix(leaderboard.features[,1:858])
leaderboard.x=model.matrix(~.,data=leaderboard.features)[,1:858]
ALSFRS.leader.pred<-predict(lasso.mod,s=bestlam,newx=leaderboard.matx) #make predictions

leaderboard.predictions$ALSFRS_slope <- ALSFRS.leader.pred
head(leaderboard.predictions)
```


Coefficients
```{r}
out=glmnet(x,y,alpha=1,lambda=grid)
lasso.coef=predict(out,type="coefficients",s=bestlam)
lasso.coef[lasso.coef!=0]
```

```{r}
out=glmnet(x,y,alpha=1,lambda=grid)
lasso.coef=predict(out,type="coefficients",s=bestlam)
lasso.coef[lasso.coef!=0]

lasso.pred=predict(lasso.coef,x,type="response")
rmse=mean((lasso.pred-y.test)^2)^.5
rmse
```

LARS: least angle regression



#####Predict ALSFRS_slope from leaderboard data and make predictions

Predict the ALSFRS_slope and load to leaderboard.predictions

```{r}
#rerun lasso.mod with all x & y (submission)
#predict with leaderboard.features (submission)
merged.data.subset=merged.data[,1:859] #add back all data

ALSFRS.leader.pred<-predict(lasso.mod,s=bestlam ,newx=leaderboard.features[,1:858]) #make predictions
leaderboard.predictions$ALSFRS_slope <- ALSFRS.leader.pred
head(leaderboard.predictions)
```

We use **write.csv** function to write a CSV file in the contest format with the leaderboard subject predictions. 

```{r}
write.csv(leaderboard.predictions, file = "leaderboard_predictions_20151023.csv",row.names=FALSE) # NEED TO MODIFY
```

